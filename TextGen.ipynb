{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGen.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1Vp9VIJrhZ4_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Dfht67wjXIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "94weaI-vhj9C",
        "colab_type": "code",
        "outputId": "4e473371-75b8-4e70-f8df-9ad33a5ab340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i_HX-QRlhpvq",
        "colab_type": "code",
        "outputId": "eeff2648-166f-4695-f8bd-bec9656dc869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# The unique characters in the file\n",
        "path_to_file ='gdrive/My Drive/HAIKU/data.tsv'\n",
        "text = open(path_to_file).read()\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96 unique characters\n",
            "'\\n'   --->    0\n",
            "' '    --->    1\n",
            "'!'    --->    2\n",
            "'\"'    --->    3\n",
            "'#'    --->    4\n",
            "'$'    --->    5\n",
            "'%'    --->    6\n",
            "'&'    --->    7\n",
            "\"'\"    --->    8\n",
            "'('    --->    9\n",
            "')'    --->   10\n",
            "'*'    --->   11\n",
            "'+'    --->   12\n",
            "','    --->   13\n",
            "'-'    --->   14\n",
            "'.'    --->   15\n",
            "'/'    --->   16\n",
            "'0'    --->   17\n",
            "'1'    --->   18\n",
            "'2'    --->   19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VoU_KjhFh3qj",
        "colab_type": "code",
        "outputId": "d4881b11-c9bb-49a4-98ec-6970dccb860a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1790
        }
      },
      "cell_type": "code",
      "source": [
        "for char,_ in zip(char2idx, range(100)):\n",
        "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\n'   --->    0\n",
            "' '    --->    1\n",
            "'!'    --->    2\n",
            "'\"'    --->    3\n",
            "'#'    --->    4\n",
            "'$'    --->    5\n",
            "'%'    --->    6\n",
            "'&'    --->    7\n",
            "\"'\"    --->    8\n",
            "'('    --->    9\n",
            "')'    --->   10\n",
            "'*'    --->   11\n",
            "'+'    --->   12\n",
            "','    --->   13\n",
            "'-'    --->   14\n",
            "'.'    --->   15\n",
            "'/'    --->   16\n",
            "'0'    --->   17\n",
            "'1'    --->   18\n",
            "'2'    --->   19\n",
            "'3'    --->   20\n",
            "'4'    --->   21\n",
            "'5'    --->   22\n",
            "'6'    --->   23\n",
            "'7'    --->   24\n",
            "'8'    --->   25\n",
            "'9'    --->   26\n",
            "':'    --->   27\n",
            "';'    --->   28\n",
            "'<'    --->   29\n",
            "'='    --->   30\n",
            "'>'    --->   31\n",
            "'?'    --->   32\n",
            "'@'    --->   33\n",
            "'A'    --->   34\n",
            "'B'    --->   35\n",
            "'C'    --->   36\n",
            "'D'    --->   37\n",
            "'E'    --->   38\n",
            "'F'    --->   39\n",
            "'G'    --->   40\n",
            "'H'    --->   41\n",
            "'I'    --->   42\n",
            "'J'    --->   43\n",
            "'K'    --->   44\n",
            "'L'    --->   45\n",
            "'M'    --->   46\n",
            "'N'    --->   47\n",
            "'O'    --->   48\n",
            "'P'    --->   49\n",
            "'Q'    --->   50\n",
            "'R'    --->   51\n",
            "'S'    --->   52\n",
            "'T'    --->   53\n",
            "'U'    --->   54\n",
            "'V'    --->   55\n",
            "'W'    --->   56\n",
            "'X'    --->   57\n",
            "'Y'    --->   58\n",
            "'Z'    --->   59\n",
            "'['    --->   60\n",
            "'\\\\'   --->   61\n",
            "']'    --->   62\n",
            "'^'    --->   63\n",
            "'_'    --->   64\n",
            "'`'    --->   65\n",
            "'a'    --->   66\n",
            "'b'    --->   67\n",
            "'c'    --->   68\n",
            "'d'    --->   69\n",
            "'e'    --->   70\n",
            "'f'    --->   71\n",
            "'g'    --->   72\n",
            "'h'    --->   73\n",
            "'i'    --->   74\n",
            "'j'    --->   75\n",
            "'k'    --->   76\n",
            "'l'    --->   77\n",
            "'m'    --->   78\n",
            "'n'    --->   79\n",
            "'o'    --->   80\n",
            "'p'    --->   81\n",
            "'q'    --->   82\n",
            "'r'    --->   83\n",
            "'s'    --->   84\n",
            "'t'    --->   85\n",
            "'u'    --->   86\n",
            "'v'    --->   87\n",
            "'w'    --->   88\n",
            "'x'    --->   89\n",
            "'y'    --->   90\n",
            "'z'    --->   91\n",
            "'{'    --->   92\n",
            "'|'    --->   93\n",
            "'}'    --->   94\n",
            "'~'    --->   95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uqJlltReih5D",
        "colab_type": "code",
        "outputId": "401bde72-72f0-4318-94a1-8ee35e948994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        " #The maximum length sentence we want for a single input in characters\n",
        "seq_length = 60\n",
        "\n",
        "# Create training examples / targets\n",
        "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in chunks.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Can't you see how much<br> better you make the world just<br>\"\n",
            "\"by being in it?\\nI'm fine. I'm listening<br> to music and eati\"\n",
            "\"ng lunch.<br>You're still an asshole.\\nYour cat has no more<br\"\n",
            "'> metaphysical value<br>than a deer or cow.\\nThis only makes m'\n",
            "'e<br> wonder where else our lovely<br>adventure can go.\\nThat '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h_KZBIHrjz8O",
        "colab_type": "code",
        "outputId": "908a0a67-621b-42b9-f013-543d9473681f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = chunks.map(split_input_target)\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  \"Can't you see how much<br> better you make the world just<br\"\n",
            "Target data: \"an't you see how much<br> better you make the world just<br>\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LlDjDwFZkYgV",
        "colab_type": "code",
        "outputId": "06cf34fc-e134-401b-8bcb-b6c4c2637b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 36 ('C')\n",
            "  expected output: 66 ('a')\n",
            "Step    1\n",
            "  input: 66 ('a')\n",
            "  expected output: 79 ('n')\n",
            "Step    2\n",
            "  input: 79 ('n')\n",
            "  expected output: 8 (\"'\")\n",
            "Step    3\n",
            "  input: 8 (\"'\")\n",
            "  expected output: 85 ('t')\n",
            "Step    4\n",
            "  input: 85 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vwclgmkFka2m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences, \n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 2000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BrbGvRUTkj10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units):\n",
        "    super(Model, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
        "                                          return_sequences=True, \n",
        "                                          recurrent_initializer='glorot_uniform',\n",
        "                                          stateful=True)\n",
        "    else:\n",
        "      self.gru = tf.keras.layers.LSTM(self.units, \n",
        "                                     return_sequences=True, \n",
        "                                     recurrent_activation='sigmoid', \n",
        "                                     recurrent_initializer='glorot_uniform', \n",
        "                                     stateful=True)\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x):\n",
        "    embedding = self.embedding(x)\n",
        "    \n",
        "    # output at every time step\n",
        "    # output shape == (batch_size, seq_length, hidden_size) \n",
        "    output = self.gru(embedding)\n",
        "    \n",
        "    \n",
        "    # The dense layer will output predictions for every time_steps(seq_length)\n",
        "    # output shape after the dense layer == (seq_length * batch_size, vocab_size)\n",
        "    prediction = self.fc(output)\n",
        "    \n",
        "    # states will be used to pass at every step to the model while training\n",
        "    return prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "elF5IXklly1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat data.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lwck5aaXlr93",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "units = 512\n",
        "\n",
        "model = Model(vocab_size, embedding_dim, units)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IAiMvU_3lzo3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using adam optimizer with default arguments\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(real, preds):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N4jBjMc1l5se",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcYvUFkOl9OC",
        "colab_type": "code",
        "outputId": "c39d9ead-9fef-427a-c066-b7fa7e702b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      multiple                  24576     \n",
            "_________________________________________________________________\n",
            "cu_dnngru_7 (CuDNNGRU)       multiple                  1182720   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              multiple                  49248     \n",
            "=================================================================\n",
            "Total params: 1,256,544\n",
            "Trainable params: 1,256,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7PIIfq6pmDnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oLETwKAmNGt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS =15\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NolZiEX8mQJL",
        "colab_type": "code",
        "outputId": "deb14aad-4d69-427f-db19-787ec7475764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3018
        }
      },
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    # initally hidden is None\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions = model(inp)\n",
        "              loss = loss_function(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix)\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5631\n",
            "Epoch 1 Batch 100 Loss 2.2776\n",
            "Epoch 1 Batch 200 Loss 2.0869\n",
            "Epoch 1 Batch 300 Loss 1.9096\n",
            "Epoch 1 Batch 400 Loss 1.7601\n",
            "Epoch 1 Batch 500 Loss 1.7199\n",
            "Epoch 1 Batch 600 Loss 1.6204\n",
            "Epoch 1 Batch 700 Loss 1.5018\n",
            "Epoch 1 Batch 800 Loss 1.5158\n",
            "Epoch 1 Batch 900 Loss 1.5423\n",
            "Epoch 1 Batch 1000 Loss 1.4891\n",
            "Epoch 1 Batch 1100 Loss 1.4964\n",
            "Epoch 1 Batch 1200 Loss 1.4918\n",
            "Epoch 1 Batch 1300 Loss 1.4381\n",
            "Epoch 1 Batch 1400 Loss 1.4198\n",
            "Epoch 1 Batch 1500 Loss 1.4315\n",
            "Epoch 1 Batch 1600 Loss 1.3753\n",
            "Epoch 1 Batch 1700 Loss 1.4284\n",
            "Epoch 1 Batch 1800 Loss 1.3684\n",
            "Epoch 1 Batch 1900 Loss 1.3885\n",
            "Epoch 1 Batch 2000 Loss 1.3582\n",
            "Epoch 1 Batch 2100 Loss 1.3558\n",
            "Epoch 1 Batch 2200 Loss 1.3363\n",
            "Epoch 1 Batch 2300 Loss 1.3198\n",
            "Epoch 1 Batch 2400 Loss 1.2835\n",
            "Epoch 1 Batch 2500 Loss 1.3313\n",
            "Epoch 1 Batch 2600 Loss 1.2945\n",
            "Epoch 1 Batch 2700 Loss 1.3301\n",
            "Epoch 1 Batch 2800 Loss 1.3174\n",
            "Epoch 1 Batch 2900 Loss 1.3111\n",
            "Epoch 1 Batch 3000 Loss 1.2875\n",
            "Epoch 1 Batch 3100 Loss 1.3949\n",
            "Epoch 1 Batch 3200 Loss 1.3149\n",
            "Epoch 1 Batch 3300 Loss 1.2831\n",
            "Epoch 1 Batch 3400 Loss 1.3045\n",
            "Epoch 1 Batch 3500 Loss 1.3035\n",
            "Epoch 1 Batch 3600 Loss 1.2282\n",
            "Epoch 1 Batch 3700 Loss 1.3048\n",
            "Epoch 1 Batch 3800 Loss 1.2843\n",
            "Epoch 1 Batch 3900 Loss 1.2931\n",
            "Epoch 1 Batch 4000 Loss 1.3030\n",
            "Epoch 1 Batch 4100 Loss 1.3178\n",
            "Epoch 1 Batch 4200 Loss 1.2986\n",
            "Epoch 1 Batch 4300 Loss 1.2481\n",
            "Epoch 1 Batch 4400 Loss 1.3122\n",
            "Epoch 1 Batch 4500 Loss 1.3026\n",
            "Epoch 1 Batch 4600 Loss 1.2742\n",
            "Epoch 1 Batch 4700 Loss 1.2756\n",
            "Epoch 1 Batch 4800 Loss 1.3032\n",
            "Epoch 1 Batch 4900 Loss 1.2417\n",
            "Epoch 1 Batch 5000 Loss 1.2836\n",
            "Epoch 1 Batch 5100 Loss 1.2997\n",
            "Epoch 1 Batch 5200 Loss 1.2752\n",
            "Epoch 1 Batch 5300 Loss 1.2204\n",
            "Epoch 1 Batch 5400 Loss 1.3033\n",
            "Epoch 1 Batch 5500 Loss 1.2327\n",
            "Epoch 1 Batch 5600 Loss 1.2381\n",
            "Epoch 1 Batch 5700 Loss 1.3728\n",
            "Epoch 1 Batch 5800 Loss 1.2167\n",
            "Epoch 1 Batch 5900 Loss 1.2912\n",
            "Epoch 1 Batch 6000 Loss 1.3250\n",
            "Epoch 1 Batch 6100 Loss 1.2649\n",
            "Epoch 1 Batch 6200 Loss 1.1995\n",
            "Epoch 1 Batch 6300 Loss 1.2104\n",
            "Epoch 1 Batch 6400 Loss 1.3026\n",
            "Epoch 1 Batch 6500 Loss 1.2971\n",
            "Epoch 1 Batch 6600 Loss 1.2859\n",
            "Epoch 1 Batch 6700 Loss 1.3043\n",
            "Epoch 1 Batch 6800 Loss 1.2481\n",
            "Epoch 1 Loss 1.2871\n",
            "Time taken for 1 epoch 482.2415268421173 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2687\n",
            "Epoch 2 Batch 100 Loss 1.2560\n",
            "Epoch 2 Batch 200 Loss 1.2189\n",
            "Epoch 2 Batch 300 Loss 1.2510\n",
            "Epoch 2 Batch 400 Loss 1.2740\n",
            "Epoch 2 Batch 500 Loss 1.2612\n",
            "Epoch 2 Batch 600 Loss 1.2404\n",
            "Epoch 2 Batch 700 Loss 1.1985\n",
            "Epoch 2 Batch 800 Loss 1.2596\n",
            "Epoch 2 Batch 900 Loss 1.2913\n",
            "Epoch 2 Batch 1000 Loss 1.2508\n",
            "Epoch 2 Batch 1100 Loss 1.2727\n",
            "Epoch 2 Batch 1200 Loss 1.1977\n",
            "Epoch 2 Batch 1300 Loss 1.2626\n",
            "Epoch 2 Batch 1400 Loss 1.2139\n",
            "Epoch 2 Batch 1500 Loss 1.2450\n",
            "Epoch 2 Batch 1600 Loss 1.2560\n",
            "Epoch 2 Batch 1700 Loss 1.2314\n",
            "Epoch 2 Batch 1800 Loss 1.2264\n",
            "Epoch 2 Batch 1900 Loss 1.1933\n",
            "Epoch 2 Batch 2000 Loss 1.2420\n",
            "Epoch 2 Batch 2100 Loss 1.2564\n",
            "Epoch 2 Batch 2200 Loss 1.1430\n",
            "Epoch 2 Batch 2300 Loss 1.2396\n",
            "Epoch 2 Batch 2400 Loss 1.2629\n",
            "Epoch 2 Batch 2500 Loss 1.2086\n",
            "Epoch 2 Batch 2600 Loss 1.2305\n",
            "Epoch 2 Batch 2700 Loss 1.2551\n",
            "Epoch 2 Batch 2800 Loss 1.1859\n",
            "Epoch 2 Batch 2900 Loss 1.2838\n",
            "Epoch 2 Batch 3000 Loss 1.2330\n",
            "Epoch 2 Batch 3100 Loss 1.2678\n",
            "Epoch 2 Batch 3200 Loss 1.2513\n",
            "Epoch 2 Batch 3300 Loss 1.2291\n",
            "Epoch 2 Batch 3400 Loss 1.2539\n",
            "Epoch 2 Batch 3500 Loss 1.2544\n",
            "Epoch 2 Batch 3600 Loss 1.2259\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-efa890176900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m               \u001b[0;31m# This is the interesting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-34e9c325ebd2>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(real, preds)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy\u001b[0;34m(labels, logits, weights, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    912\u001b[0m     losses = nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n\u001b[1;32m    913\u001b[0m                                                          \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m                                                          name=\"xentropy\")\n\u001b[0m\u001b[1;32m    915\u001b[0m     return compute_weighted_loss(\n\u001b[1;32m    916\u001b[0m         losses, weights, scope, loss_collection, reduction=reduction)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   2633\u001b[0m     \u001b[0;31m# Store label shape for result later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mlabels_static_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2635\u001b[0;31m     \u001b[0mlabels_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2636\u001b[0m     static_shapes_fully_defined = (\n\u001b[1;32m   2637\u001b[0m         \u001b[0mlabels_static_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m   \"\"\"\n\u001b[0;32m--> 309\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   7988\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   7989\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7990\u001b[0;31m         _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m   7991\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7992\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "hai50Sdrp9rf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(checkpoint_prefix)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "75k7dMsdqNBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls {checkpoint_dir}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_203jaORqShY",
        "colab_type": "code",
        "outputId": "4c89b877-25d7-4d3b-939c-22ce29a7fd52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "metadata": {
        "id": "uGTfm80jqiiT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(vocab_size, embedding_dim, units)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TsKATyGRqxZz",
        "colab_type": "code",
        "outputId": "644cfb93-af34-4159-f715-e4cc1a74ba5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "num_generate = 12\n",
        "# You can change the start string to experiment\n",
        "start_string = 'I failed'\n",
        "\n",
        "# Converting our start string to numbers (vectorizing) \n",
        "input_eval = [char2idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# Empty string to store our results\n",
        "text_generated = []\n",
        "\n",
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "temperature = 0.9\n",
        "\n",
        "# Evaluation loop.\n",
        "\n",
        "# Here batch size == 1\n",
        "model.reset_states()\n",
        "count_line=0\n",
        "\n",
        "while ((count_line<2) or (not( idx2char[predicted_id]=='.'))):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a multinomial distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "    if predicted_id==29:\n",
        "      count_line+=1;\n",
        "      \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "print ((start_string + ''.join(text_generated))[:])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I failed her.\n",
            "No just to use<br> different as hell if you should<br>do anything? Why?\n",
            "I read the one<br> has a fucking look or you<br>really seen these much.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hGms_swxqy2K",
        "colab_type": "code",
        "outputId": "c30f2d6b-dfef-4564-8c5e-6df35204e500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I failed the same<br> and see what the same thing that's<br>about to see that.\n",
            "The only really<br>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}